{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Colab version to train on T4 GPU:\n",
        "[https://colab.research.google.com/drive/1TRE6DIhLm5P6k1bdFxXlRpOvzfalH4zC#scrollTo=xfosNdAaoWeH](https://colab.research.google.com/drive/1TRE6DIhLm5P6k1bdFxXlRpOvzfalH4zC#scrollTo=xfosNdAaoWeH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KiVX9QR0oEL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "import av\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from bayesian_torch.models.dnn_to_bnn import dnn_to_bnn, get_kl_loss\n",
        "from bayesian_torch.layers.variational_layers.linear_variational import LinearReparameterization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdKvGfXf08NF",
        "outputId": "6e81576b-c2c0-496d-81e7-2a0374ee8128"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] Loaded 3012 videos with top 100 labels\n",
            "[TEST] Loaded 458 videos with top 100 labels\n",
            "[VALIDATION] Loaded 815 videos with top 100 labels\n"
          ]
        }
      ],
      "source": [
        "from utils.dataset import load_msasl\n",
        "\n",
        "label_threshold = 100\n",
        "test_dataset, train_dataset, validation_dataset = load_msasl('bin', label_threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch of videos: torch.Size([8, 64, 3, 224, 224])\n",
            "Batch of labels: torch.Size([8])\n",
            "Metadata sample: {'id': ['4e27d655-a1c3-4e18-88a7-66d54a79faa8', 'b69acd18-b57e-4ecd-b34c-58c41350d2ca', 'a8a79544-1d0d-4200-a1c6-bdc96bcc1faf', '043f4248-8c6b-417c-8802-cc46db558491', 'edfd9c7b-5332-402d-bd37-c92fcf754118', '44bfe295-d98a-4752-900e-4d7742817267', '7f605d99-a586-4f22-96b1-c8dee1dc9fbc', '72610c19-ef0d-412d-bb81-d47e911fa7a9'], 'org_text': ['pink ', 'FAMILY', 'learn', 'boy cousin(male location)', 'FOOD', 'SICK', 'what', 'wish'], 'clean_text': ['pink', 'family', 'learn', 'cousin', 'eat', 'sick', 'what', 'wish'], 'signer_id': tensor([144,  72,  40, 107, 124,  77,  40,  36]), 'signer': tensor([-1, 23, -1,  2, -1, -1, -1, 89]), 'file': ['pink - ASL sign for pink', 'Unit 08 Vocabulary', 'ActionsVerbs in school ASL', 'Unit 4 vocabulary pg 1-3', 'Major Exports Vocabulary  ASL - American Sign Language', 'Unit 6 Vocabularymp4', 'Robber and the Geek Vocab video for ASL class', 'wish'], 'label': tensor([96, 44, 22, 33,  3, 63, 15, 54]), 'fps': tensor([25, 25, 25, 25, 25, 25, 25, 25]), 'url': ['https://www.youtube.com/watch?v=8ZXnFKV-SwA', 'https://www.youtube.com/watch?v=bq-HmgjGzmw', 'https://www.youtube.com/watch?v=OB1xBzqD8WY', 'https://www.youtube.com/watch?v=12Cqpj9g96Q', 'https://www.youtube.com/watch?v=hUrfB8bikfw', 'https://www.youtube.com/watch?v=fKjsdtMU3fc', 'https://www.youtube.com/watch?v=z8e_-viWx9E', 'https://www.youtube.com/watch?v=fqGkgtkX7Qk'], 'text': ['pink', 'family', 'learn', 'cousin', 'eat', 'sick', 'what', 'hungry'], 'filename': ['pink_4e27d655-a1c3-4e18-88a7-66d54a79faa8', 'family_b69acd18-b57e-4ecd-b34c-58c41350d2ca', 'learn_a8a79544-1d0d-4200-a1c6-bdc96bcc1faf', 'cousin_043f4248-8c6b-417c-8802-cc46db558491', 'eat_edfd9c7b-5332-402d-bd37-c92fcf754118', 'sick_44bfe295-d98a-4752-900e-4d7742817267', 'what_7f605d99-a586-4f22-96b1-c8dee1dc9fbc', 'wish_72610c19-ef0d-412d-bb81-d47e911fa7a9']}\n",
            "tensor([91, 38, 45,  4,  0, 46, 19, 28])\n",
            "Batch of videos: torch.Size([8, 64, 3, 224, 224])\n",
            "Batch of labels: torch.Size([8])\n",
            "Metadata sample: {'id': ['e8592ba7-80b8-4b82-baa7-9fc8514ec020', '24ad6c36-6a0c-4797-9f10-3a7f605c4efe', '62a7ba96-c1f0-4033-9b88-39ff64158491', '8141c74c-d106-4963-93bf-955aced7f8b7', '6e0f80c7-75ac-4658-ba57-d5a642db7dd0', '4229028a-4392-4f94-8362-86586d642768', '12c8a305-7d4e-484f-b4cd-404d494ae821', '5366ef2a-0e3a-44e2-b5ef-b5c5969777f6'], 'org_text': ['teacher', 'Happy', 'BOOK', 'SIT', 'hurt', 'none', 'LIKE', 'DIFFERENT'], 'clean_text': ['teacher', 'happy', 'book', 'sit', 'hurt', 'nothing', 'like', 'different'], 'signer_id': tensor([125,  73, 140, 261, 411, 125, 265,   3]), 'signer': tensor([ 8, 53, 36, 10, 25,  8, 25, -1]), 'file': ['ASL I - Unit 2 - Vocabulary', 'How to Sign Happy Mothers Day in Sign Language MOTHERS DAY 2017', 'ASL - Activities in School', 'ASL Basic Classroom Signs', 'Master ASL Unit 8 (captions available click on CC bottom right)', 'ASL2 Unit 13', 'ASL1Chapter2 Vocab', 'DIFFERENT(3)'], 'label': tensor([ 2,  5, 38, 18, 76, 37,  6, 55]), 'fps': tensor([25, 25, 25, 25, 25, 25, 25, 25]), 'url': ['https://www.youtube.com/watch?v=0bIF7jh6lnE', 'https://www.youtube.com/watch?v=qysriDtUS5o', 'https://www.youtube.com/watch?v=afwqCG7GmrQ', 'https://www.youtube.com/watch?v=bX1eJjB3nyA', 'https://www.youtube.com/watch?v=2ofPs_uTel4', 'https://www.youtube.com/watch?v=dELt8wh_5nA&t=91s', 'https://www.youtube.com/watch?v=cxXEULq9Jpc', 'https://www.youtube.com/watch?v=abThwavniFg'], 'text': ['teacher', 'happy', 'book', 'sit', 'hurt', 'nothing', 'like', 'but'], 'filename': ['teacher_e8592ba7-80b8-4b82-baa7-9fc8514ec020', 'happy_24ad6c36-6a0c-4797-9f10-3a7f605c4efe', 'book_62a7ba96-c1f0-4033-9b88-39ff64158491', 'sit_8141c74c-d106-4963-93bf-955aced7f8b7', 'hurt_6e0f80c7-75ac-4658-ba57-d5a642db7dd0', 'nothing_4229028a-4392-4f94-8362-86586d642768', 'like_12c8a305-7d4e-484f-b4cd-404d494ae821', 'different_5366ef2a-0e3a-44e2-b5ef-b5c5969777f6']}\n",
            "tensor([ 7, 74, 56, 69, 47, 59, 17, 68])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "data_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
        "for videos, labels, metadata in data_loader:\n",
        "        print(f\"Batch of videos: {videos.shape}\") # (batch_size, 64, C, H, W)\n",
        "        print(f\"Batch of labels: {labels.shape}\") # (batch_size,)\n",
        "        print(f\"Metadata sample: {metadata}\") # Dictionary of metadata\n",
        "        print(labels)\n",
        "        break # Checking the first batch\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
        "\n",
        "\n",
        "\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
        "\n",
        "for videos, labels, metadata in validation_loader:\n",
        "        print(f\"Batch of videos: {videos.shape}\") # (B, T, C, H, W)\n",
        "        print(f\"Batch of labels: {labels.shape}\") # (batch_size,)\n",
        "        print(f\"Metadata sample: {metadata}\") # Dictionary of metadata\n",
        "        print(labels)\n",
        "        break # Checking the first batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8K4vii_N1IK1"
      },
      "outputs": [],
      "source": [
        "# test_dataset.show_video(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Print Train Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjheFoRKoVQa",
        "outputId": "dfc312fd-52e9-4d25-ee1d-b2f59f1c70a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num of GPU: 1\n",
            "_CudaDeviceProperties(name='NVIDIA GeForce RTX 3060 Laptop GPU', major=8, minor=6, total_memory=6143MB, multi_processor_count=30, uuid=a43e09d2-abbb-44a0-a8cb-9ebfcebe6d64, L2_cache_size=3MB)\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'num of GPU: {torch.cuda.device_count()}')\n",
        "    print(torch.cuda.get_device_properties(0))\n",
        "else:\n",
        "    print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xfosNdAaoWeH",
        "outputId": "a790d7f4-5cb8-4095-d8c6-97f813d90ee5"
      },
      "outputs": [],
      "source": [
        "from model.RetNet18_GRU import ResNet18_GRU\n",
        "\n",
        "cnn_gru_model = ResNet18_GRU()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert FC Layer to Bayesian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet18_GRU(\n",
              "  (feature_extractor): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  )\n",
              "  (gru): GRU(512, 20, num_layers=2, batch_first=True)\n",
              "  (fc): LinearReparameterization()\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# convert to bayesian\n",
        "ori_in_features = cnn_gru_model.fc.in_features\n",
        "ori_out_features = cnn_gru_model.fc.out_features\n",
        "\n",
        "cnn_gru_model.fc = LinearReparameterization(in_features=ori_in_features,\n",
        "                                out_features=label_threshold,\n",
        "                                prior_mean=0,\n",
        "                                prior_variance=1,\n",
        "                                posterior_mu_init=0,\n",
        "                                posterior_rho_init=-3.0,\n",
        "                                bias=True)\n",
        "\n",
        "cnn_gru_model.fc.dnn_to_bnn_flag = True\n",
        "\n",
        "cnn_gru_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AALnMoD3LBoN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/12:   0%|          | 0/377 [01:11<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m videos = videos.float() / \u001b[32m255.0\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m outputs = \u001b[43mcnn_gru_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Check NaN values\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m cnn_gru_model.named_parameters():\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/cs5340-uncertainty-modelling/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/cs5340-uncertainty-modelling/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/cs5340-uncertainty-modelling/model/RetNet18_GRU.py:27\u001b[39m, in \u001b[36mResNet18_GRU.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Reshape for CNN: Process (B*T, C, H, W) through ResNet18\u001b[39;00m\n\u001b[32m     26\u001b[39m x = x.view(B * T, C, H, W)  \u001b[38;5;66;03m# Merge batch & time\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass through ResNet18 (CNN)\u001b[39;00m\n\u001b[32m     28\u001b[39m x = x.view(B, T, -\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Reshape to (B, T, Feature_dim)\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Pass through LSTM\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/cs5340-uncertainty-modelling/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/cs5340-uncertainty-modelling/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/cs5340-uncertainty-modelling/venv/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/cs5340-uncertainty-modelling/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/cs5340-uncertainty-modelling/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/cs5340-uncertainty-modelling/venv/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/cs5340-uncertainty-modelling/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/cs5340-uncertainty-modelling/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/cs5340-uncertainty-modelling/venv/lib/python3.12/site-packages/torchvision/models/resnet.py:92\u001b[39m, in \u001b[36mBasicBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m     90\u001b[39m     identity = x\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.bn1(out)\n\u001b[32m     94\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.relu(out)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/cs5340-uncertainty-modelling/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/cs5340-uncertainty-modelling/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/cs5340-uncertainty-modelling/venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/cs5340-uncertainty-modelling/venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer (Adam, no learning rate scheduler)\n",
        "optimizer = optim.Adam(cnn_gru_model.parameters(), lr=0.001)  # No scheduler\n",
        "\n",
        "# Scheduler\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "num_epochs = 12  # Adjust based on performance\n",
        "batch_size = 8\n",
        "\n",
        "# Directory to save models and results\n",
        "output_dir = \"model_outputs\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def check_nan(tensor, name):\n",
        "    if torch.isnan(tensor).any() or torch.isinf(tensor).any():\n",
        "        print(f\"⚠️ NaN or Inf detected in {name}!\")\n",
        "\n",
        "results_file = os.path.join(output_dir, \"training_metrics.txt\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    cnn_gru_model.train()  # Set model to training mode\n",
        "    total_loss = 0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    train_loader_tqdm = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n",
        "\n",
        "    for videos, labels, metadata in train_loader_tqdm:\n",
        "        videos, labels = videos.to(device), labels.to(device)\n",
        "\n",
        "        # Reset gradients\n",
        "        optimizer.zero_grad()\n",
        "        videos = videos.float() / 255.0\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = cnn_gru_model(videos)\n",
        "\n",
        "        # Check NaN values\n",
        "        for name, param in cnn_gru_model.named_parameters():\n",
        "            check_nan(param, f\"Param {name}\")\n",
        "\n",
        "        # Compute loss\n",
        "        kl = get_kl_loss(cnn_gru_model)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss = loss + kl / batch_size\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient Clipping\n",
        "        for name, param in cnn_gru_model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                check_nan(param.grad, f\"Grad {name}\")\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(cnn_gru_model.parameters(), max_norm=5) # Gradient clipping\n",
        "\n",
        "        # Update model weights\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        train_loader_tqdm.set_postfix(loss=loss.detach().item())  # Update tqdm display\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "    # Validation step\n",
        "    cnn_gru_model.eval()\n",
        "    correct_val, total_val = 0, 0\n",
        "\n",
        "    # Wrap validation loader with tqdm for validation progress\n",
        "    val_loader_tqdm = tqdm(validation_loader, desc=\"Validating\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for videos, labels, metadata in val_loader_tqdm:\n",
        "            videos = videos.to(device).float() / 255.0 # Convert videos to float32\n",
        "            labels = labels.to(device).long()   # Convert labels to long\n",
        "            outputs = cnn_gru_model(videos)\n",
        "            predicted_val = torch.argmax(outputs, dim=1)\n",
        "            correct_val += (predicted_val == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # Save the model after each epoch\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\")\n",
        "    model_name = f\"cnn_gru_epoch_{epoch+1}_{timestamp}.pth\"\n",
        "    model_path = os.path.join(output_dir, model_name)\n",
        "    torch.save(cnn_gru_model.state_dict(), model_path)\n",
        "    print(f\"Model saved to: {model_path}\")\n",
        "\n",
        "    # Output metrics to a text file\n",
        "    with open(results_file, \"a\") as f:\n",
        "        f.write(f\"Epoch: {epoch+1}, Loss: {avg_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%, Model Name: {model_name}\\n\")\n",
        "\n",
        "    # scheduler.step()\n",
        "\n",
        "print(\"Training finished. Results saved to:\", results_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test set evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final evaluation on the test set\n",
        "cnn_gru_model.eval()\n",
        "correct_test, total_test = 0, 0\n",
        "test_loader_tqdm = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for videos, labels, metadata in test_loader_tqdm:\n",
        "        videos = videos.to(device).float() / 255.0\n",
        "        labels = labels.to(device).long()\n",
        "        outputs = cnn_gru_model(videos)\n",
        "        predicted_test = torch.argmax(outputs, dim=1)\n",
        "        correct_test += (predicted_test == labels).sum().item()\n",
        "        total_test += labels.size(0)\n",
        "\n",
        "test_accuracy = 100 * correct_test / total_test\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "with open(results_file, \"a\") as f:\n",
        "    f.write(f\"Final Test Accuracy: {test_accuracy:.2f}%\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
